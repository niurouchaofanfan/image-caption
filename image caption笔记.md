# image caption笔记

### 基于模板的image caption方法

> 模板是预定义的且该方法只能生成固定长度的图像标题

### 基于检索的image caption方法（retrieval-based）

>在基于检索的方法中，字幕是从一组现有的字幕中检索出来的。基于检索的方法首先从训练数据集中找到具有标题的视觉相似的图像。这些标题称为候选标题。查询图像的标题是从这些标题池中选择的

>缺点是：只能生成语法正确的标题，不能生成语义正确的标题

### novel标题

> 这一类别的一般方法是首先分析图像的视觉内容，然后使用语言模型从视觉内容生成图像标题。

> 这类方法大多使用深度学习的方法，效果比先前的好

#### 图1描述了基于深度学习的图像字幕方法的总体分类。

<img src="1.png" alt="图1描述了基于深度学习的图像字幕方法的总体分类。" style="zoom:50%;" />

> 该图说明了不同类别的图像字幕方法的比较。基于图像字幕生成的图像字幕方法多采用视觉空间和基于深度机器学习的技术。字幕也可以从多模式空间生成。 基于深度学习的图像字幕方法也可以归类为以下学习技术：监督学习，强化学习和无监督学习。

## Visual Space vs. Multimodal Space

> 在基于视觉空间的方法中，图像特征和相应的字幕独立地传递给语言解码器。相比之下，在多模态空间中，共享多模态空间是从图像和相应的标题文本中学习的。然后将这个多模态表示传递给语言解码器。

#### 多模态空间

> 典型的多模态空间方法的体系结构包括语言编码器部分、视觉部分、多模态空间部分和语言解码器部分

> 视觉部分使用深度卷积神经网络作为特征提取器来提取图像特征。 语言编码器部分提取单词特征，并为每个单词学习密集的特征嵌入。 然后将语义时态上下文转发到递归层。多模态空间部分将图像特征映射到带有词特征的公共空间。然后，生成的映射被传递给语言解码器，后者通过解码映射生成标题。

>此类方法包括以下步骤：
>
>（1）使用深度神经网络和多模态神经语言模型在多模态空间中共同学习图像和文本。
>
>（2）语言生成部分使用步骤1中的信息生成字幕。

![基于空间的多模态图像字幕方法的总图如图2所示。](2.png)

​												基于空间的多模态图像字幕方法的总图如图2所示。

> Kiros et al. [69].的工作利用CNN提取图像特征，生成图像标题。它使用一个多模态空间来联合表示图像和文本，用于多模态表示学习和图像字幕生成。介绍了多模态神经语言模型。
>
> 与大多数以前的方法不同，此方法不依赖任何其他模板，结构或约束。 相反，它取决于分别从深度神经网络和多模式神经语言模型中学到的高级图像特征和单词表示。 神经语言模型有局限性，无法处理大量数据，并且无法有效地长期存储[64]。
>
> Kiros等。 [70]扩展了他们在[69]中的工作，学习了联合图像句子嵌入，其中LSTM用于句子编码，而新的神经语言模型称为**结构内容神经语言模型（****SC-NLM）用于图像字幕生成**。与现有方法相比，SC-NLM具有一个优点，即它**可以将句子的结构提取到编码器生成的内容中**。与[69]提出的方法相比，它还可以帮助他们在生成逼真的图像标题方面取得重大改进。

##### Mao等人[94]提出了一种多模态递归神经网络(m-RNN)方法

> 该方法有两个子网：用于语句的深度递归神经网络和用于图像的深度卷积网络。 这两个子网在多模式层中相互交互以形成整个m-RNN模型。 图像和句子片段都作为此方法的输入。 它计算概率分布以生成字幕的下一个单词。

> 此模型中还有五层：两个字嵌入层，循环层，多模态层和SoftMax层

> Kiros等。 [69]提出了一种基于对数-双线性模型的方法，并使用AlexNet提取视觉特征。 这种多模态递归神经网络方法与Kiros等人的方法密切相关。 [69]。 Kiros等。 使用固定长度的上下文（即五个词），而在此方法中，时间上下文存储在循环架构中，该架构允许任意上下文长度。

> 两个词嵌入层使用一个one-hot向量生成一个密集的词表示。它对这些词的句法意义和语义意义进行编码。通过计算嵌入层中两个密集词向量之间的欧氏距离，可以得到语义相关的词。

> 大多数句子-图像多模态方法[38,666,70,128]使用预先计算的词嵌入向量来初始化它们的模型。与此相反，**该方法随机初始化单词嵌入层，并从训练数据中学习它们。**这有助于他们产生更好的图像标题比以前的方法。**目前许多图像标注方法****[666,69,95]都是建立在递归神经网络上的。他们使用一个循环层来存储视觉信息。**但是，**（****m-RNN）同时使用图像表示和句子片段来生成标题。 它可以更有效地利用循环层的容量，从而有助于使用尺寸较小的循环层实现更好的性能。

## Supervised Learning vs. Other Deep Learning

> 在监督学习中，训练数据带有期望的输出，称为标签。 另一方面，无监督学习则处理未标记的数据。 生成对抗网络（GAN）[48]区域类型的无监督学习技术。 强化学习是另一种机器学习方法，其中代理的目的是通过探索和奖励信号发现数据和/或标签。 许多图像字幕方法使用强化学习和基于GAN的方法。

#### GAN方法

>基于GAN的方法可以从未标记的数据中学习深度特征。他们通过在两个网络(发生器和鉴别器)之间应用竞争过程来实现这种反馈。GAN已成功用于各种应用中，包括图像标注[26,126]，图像到图像翻译[56]，文本到图像合成[15,115]和文本生成[36，145]。

> GAN有两个问题。首先，GAN可以很好地从真实图像生成自然图像，因为它被用于实值数据。**然而，文本处理是基于离散数字的。因此，这些操作是不可微的，因此很难直接应用反向传播。**策略梯度应用一个参数函数来允许梯度反向传播。其次，评估者面临梯度消失和序列生成误差传播的问题。 每个局部描述都需要一个可能的未来奖励值。MonteCarlo展示[157]用于计算该未来奖励值。

> 与传统的深度卷积网络和基于深度递归网络模型相比，基于GAN的图像字幕方法可以生成多种图像字幕。Dai等人也提出了一种基于GAN的图像字幕方法。但是，他们不考虑单个图像的多个字幕。 Shetty等。 [126]引入了一种新的基于GAN的图像字幕方法。该方法可以为一幅图像生成多个标题，生成不同的字幕的改进效果显著。GANs在反向传播离散数据方面有局限性。Gumbel sampler[58,91]是用来克服离散数据问题的。这个对抗网络的两个主要部分是生成器和鉴别器。在训练中，生成器学习鉴别器提供的损失值，而不是从外部来源学习。鉴别器具有真实的数据分布，能够区分生成者生成的样本和真实的数据样本。这使得网络可以学习不同的数据分布。此外，该网络将生成的字幕集分为真字幕集和假字幕集。因此，它可以生成类似于人类生成的标题。

### Dense Captioning vs. Captions for the whole scene

> dense caption是为场景中的每一个区域分别生成标题，其他方法是为整个场景生成一个标题。

---

#### 3.3.1Dense Captioning.

> 此前的方法都是为整个场景生成标题，这样的结果并不好。
>
> 该类别的典型方法包括以下步骤：
>
> （1）为给定图像的不同区域生成区域建议;
>
> （2）使用CNN获得基于区域的图像特征。
>
> （3）步骤2的输出作为语言模型的输入来为每个区域生成字幕。

> Johnson et al. [62] 提出了一个叫DenseCap的方法，该方法首先定位出所有重要的区域(类似目标检测)，然后分别对这些区域生成标题。Dense captioning [62] 提出了一个全卷积定位的网络架构，包含一个卷积网络、一个密集的定位层和一个LSTM[54]语言模型。密集的定位层通过单个有效的前向通过来处理图像，这隐含预测了图像中的一组感兴趣区域。(类似于目标检测的proposal区域)因此，与快速R-CNN或快速R-CNN的完整网络（即RPN（区域提议网络[44]）不同），它不需要外部区域建议。定位层的工作原理与FasterR-CNN的工作有关[116]。然而，Johnson等人[62]使用差动空间软注意机制[49,57]和双线性插值[57]代替ROI池机制[44]。这种改变帮助该方法在网络中进行反向传播，平滑地选择活动区域。它使用Visual Genome[72]数据集进行区域级图像字幕生成实验。

![](D:\研究生文献\图像理解\3.png)

​														图4给出了一种典型的密集字幕方法的框图。

#### dense caption优点

> 对整个视觉场景的一种描述是相当主观的，不足以让人完全理解。**基于区域的描述比全局图像描述更客观、更详细**。基于区域的描述称为密集字幕。

#### dense caption面临的挑战及解决

> 当区域密集时，一个对象可能具有多个感兴趣的重叠区域。 而且，很难为所有视觉概念识别每个目标区域。

> Yang et al. [153] 提出的dense caption方法解决了这个问题，首先，它介绍了一种推理机制，该机制共同取决于该区域的视觉功能和该区域的预测字幕。 这允许模型找到边界框的适当位置。 其次，他们应用了上下文融合，可以将上下文特征与相应区域的视觉特征结合起来以提供丰富的语义描述。

---

#### 3.3.2Captions for the whole scene.

> 编码器-解码器体系结构，组合体系结构，基于注意力，基于语义概念的风格化字幕，基于新颖对象的图像字幕以及其他基于深度学习网络的图像字幕方法会为整个场景生成单个或多个字幕。

---

---

## 3.4  Encoder-Decoder Architecture vs. Compositional Architecture

> 有些方法只使用简单的编码器和解码器来生成标题。然而，其他方法使用多个网络。

#### 3.4.1    Encoder-Decoder Architecture-Based Image captioning.

> 基于神经网络的图像标注方法仅以简单的端到端方式工作。 这些方法与基于编码器-解码器框架的神经机器翻译非常相似[131]。 在该网络中，**从CNN的隐藏激活中提取了全局图像特征，然后将它们输入到LSTM中以生成单词序列。**

> 此类的典型方法包括以下一般步骤：
>
> （1）使用一个普通的CNN网络获取场景类型，以检测对象及其关系。
>
> （2）语言模型使用步骤1的输出进行转换 它们变成单词，组合短语产生图像标题。

![](D:\研究生文献\图像理解\4.png)

​																图5给出了此类的简单框图。

> Vinyals et al. [142]提出了一个叫NIC的方法，该方法使用CNN进行图像表示，并使用LSTM生成图像字幕。 这个特殊的CNN使用新颖的方法进行批量标准化，并将CNNis的最后一个隐藏层的输出用作LSTM解码器的输入。 该LSTM能够跟踪已经使用文本描述的对象。 NIC是基于最大似然估计进行训练的。

> 在生成图像标题时，图像信息被包含到LSTM的初始状态。 然后根据当前时间步长和先前的隐藏状态生成下一个单词。这个过程持续到获得句子的结尾标记。 由于图像信息仅在处理开始时才送入，因此可能会面临消失的梯度问题。 一开始产生的单词的作用也越来越弱。 因此，**LSTM在生成长句子方面仍然面临挑战[7,24]**。因此，**Jia等人 [59]提出了LSTM的扩展，称为guide-LSTM（gLSTM）。** 此gLSTM可以生成长句子。 在这种体系结构中，它将全局语义信息添加到LSTM的每个门和单元状态。 它还考虑了不同的长度归一化策略来控制字幕的长度。 语义信息是通过不同方式提取的。 首先，它使用跨模式检索任务来检索图像字幕，然后从这些字幕中提取语义信息。 还可以使用多模态嵌入空间来提取基于语义的信息。

> mao [92]提出了一种特殊类型的图像文本生成方法。 该方法可以生成对特定对象或区域的描述，称为引用表达式[37,46,68,102,103,136,141]。 然后，使用该表达式可以推断出正在描述的对象或区域。 因此，生成的描述或表达是非常明确的。 为了解决引用表达式，该方法使用了基于流行的MS COCO数据集的称为ReferIt数据集[68]的新数据集。

> **以前的基于CNN-RNN的图像字幕方法使用LSTM，它们是单向的并且深度相对较浅。** 在单向语言生成技术中，根据视觉上下文和所有先前的文本上下文来预测下一个单词。 单向LSTM无法生成上下文格式正确的字幕。 此外，最近的目标检测和分类方法[73,127]表明，深度的，分层的方法比浅的方法更易于学习。
>
> Wang等。 [144]提出了一种基于**深度双向LSTM**的图像字幕方法。 **该方法能够生成上下文和语义丰富的图像标题。** 所提出的体系结构由CNN和两个单独的LSTM网络组成。 它可以利用过去和将来的上下文信息来学习长期的视觉语言交互。

---

#### 3.4.2    Compositional Architecture-Based Image captioning.

> 基于组件架构的方法由几个独立的功能构件组成:首先，使用CNN从图像中提取语义概念。然后使用语言模型生成一组候选标题。在生成最终标题时，这些候选标题使用一个深度多模态相似模型重新排序。

> 该类别的典型方法包括以下步骤：
>
> （1）使用CNN获得图像特征。
>
> （2）从视觉特征获得视觉概念（例如属性）。
>
> （3）语言模型使用 步骤1和步骤2的信息。
>
>   (4)   利用深度多模态相似度模型对生成的字幕进行重新排序，选择高质量的图像字幕。

![](D:\研究生文献\图像理解\5.png)

​												图6给出了基于合成网络的图像字幕方法的一般框图。

> Fang等人提出了基于生成的图像字幕。它使用视觉检测器、语言模型和多模态相似模型在图像字幕数据集上训练模型。图像标题可以包含名词，动词和形容词。 使用来自训练标题的1000个最常见的单词来形成词汇表。 该系统使用图像子区域而不是整个图像。卷积神经网络（AlexNet [73]和VGG16Net）都用于提取图像子区域的特征。**子区域的特征与可能包含在图像标题中的词汇词相对应。多实例学习****(multi instance learning, MIL)[96]****用于训练学习每个单词的识别性视觉特征的模型。使用最大熵****[12]****语言模型从这些单词生成图像标题。**生成的字幕通过句子特征的线性加权进行排序。 最小错误率训练（MERT）[106]用于学习这些权重。图像和句子之间的相似度可以很容易地用一个公共向量表示来测量。利用深度多模态相似模型(DMSM)将图像和句子片段用公共向量表示。它在选择高质量的图像字幕方面取得了显著的进步。

> 到目前为止，已有大量的方法在生成图像字幕方面取得了令人满意的进展。这些方法使用来自同一领域的训练和测试样本。因此，**不能确定这些方法在开放域图像中是否能很好地执行。**此外，他们只擅长识别一般的视觉内容。**有些关键的实体，如名人和地标，不在他们的范围之内。**这些方法生成的标题是对BLEU[110]、METEOR[1]和CIDEr[139]等自动指标的评价。这些评估指标已在这些方法上显示出良好的结果

> Tran等人。 [135]引入了另一种图像字幕方法。 **此方法甚至能够为其他领域(开放域)图像生成图像标题。**它可以检测到各种各样的视觉概念，并为名人和地标生成字幕。 它使用外部知识库Freebase [16]来识别各种各样的实体，例如名人和地标。一系列人工判断被用于评估所生成字幕的性能。 在实验中，它使用三个数据集：MS COCO，Adobe-MIT FiveK [19]和来自Instagram的图像。 MS COCOdataset的图像是从同一域中收集的，而其他数据集的图像是从一个开放域中选择的。特别是在具有挑战性的Instagram数据集上，该方法取得了显著的性能。

> Wang等[146]提出了一种用于图像字幕生成的**并行融合RNN-LSTM结构**。该方法的体系结构将RNN和LSTM的隐藏单元划分为若干大小相同的部分。各部分按相应比例并行工作，生成图像字幕。

---

## Others

### 3.5.1Attention based Image Captioning.

> 基于神经编码器-解码器的方法主要用于机器翻译[131]。 遵循这些趋势，它们也已用于图像字幕任务，并且非常有效。 **在图像字幕中，CNN用作编码器以从输入图像中提取视觉特征，RNN用作解码器以将该表示形式逐字转换为图像的自然语言描述。**但是，**这些方法在生成图像描述时无法随时间分析图像。** 除此之外，这些方法不考虑与图像字幕部分相关的图像空间方面的信息。 相反，它们会考虑整个场景来生成字幕。

> **基于注意力的机制可以解决这些局限性，因此在深度学习中正变得越来越流行。 在生成输出序列时，它们可以动态地专注于输入图像的各个部分。**

>此类别的典型方法采用以下步骤：
>
>（1）通过CNN获得基于整个场景的图像信息。
>
>（2）语言生成阶段使用步骤1的输出生成单词或短语。
>
>（3）基于生成的单词或短语，在语言生成模型的每个时间步中聚焦给定图像的显著区域。（4）字幕会动态更新，直到语言生成模型的最终状态为止。

![](D:\研究生文献\图像理解\6.png)

​										基于注意力的图像字幕方法的框图如图7所示。

> 徐等。 [152]是第一个引入基于注意力的图像字幕方法的人。 该方法自动描述图像的显着内容。基于注意力的方法与其他方法之间的主要区别在于，**它们可以专注于图像的显着部分并同时生成相应的单词**。 该方法应用了两种不同的技术：**随机的硬注意力和确定性的软注意力以产生注意力**。大多数基于CNN的方法使用卷积神经网络的顶层从图像中提取显着对象的信息。 这些技术的一个缺点是它们可能会丢失某些信息，这些信息对于生成详细的字幕非常有用。 **为了保留信息，注意力方法使用来自较低卷积层的特征，而不是全连接层。**

> Jin等。 [61]提出了另一种基于注意力的图像字幕方法。 该方法能够基于视觉信息和文本信息之间的语义关系提取抽象含义的流。它还可以通过提出场景特定的上下文来获得更高层次的语义信息。该方法与其他基于注意的方法的主要区别在于，它在多个尺度上引入了图像的多个视觉区域。这种技术可以提取特定物体的适当的视觉信息。为了提取场景特定的上下文，它首先使用潜在的Dirichlet分配(LDA)[14]从数据集的所有标题生成一个字典。然后利用多层感知器预测每个图像的主题向量。场景分解的LSTM有两个堆叠的层，用于生成图像整体上下文的描述。

> Wu等人[151]提出了一种基于评论的图像字幕注意方法。它引入了一个review模型，该模型可以在关注CNN隐藏状态的情况下执行多个审查步骤。CNN的输出是一些事实向量，这些事实向量可以获得图像的全局事实。这些参数作为LSTM注意机制的输入。例如，一个审查器模块首先审查:图像中的对象是什么?然后，它可以检查对象的相对位置，另一个检查可以提取图像整体上下文的信息。这些信息被传递给解码器以生成图像标题。

> Pedersoli等。 [112]提出了一种**基于区域的图像字幕关注机制**。 基于先前注意力的方法仅将图像区域映射到RNN语言模型的状态。 但是，**此方法将图像区域与给定RNN状态的字幕单词相关联。** 它可以在RNN的每个时间步中预测下一个字幕词和相应的图像区域。 它能够在RNN的每个时间步中预测下一个单词以及相应的图像区域，以生成图像字幕。**为了找到注意区域**，以前的基于注意的图像标题方法要么使用CNN激活网格的位置，要么使用对象建议。与此相反，**该方法使用了一个端到端可训练的卷积空间变换器以及CNN的激活网格和对象建议方法。**这些技术的组合有助于此方法计算图像**自适应关注区域**。 在实验中，该方法表明，这种新的注意力机制与空间变换器网络一起可以产生高质量的图像字幕。

> Lu等[88]提出了另一种基于注意力的图像字幕方法。该方法是基于自适应注意力模型的视觉标记方法。当前基于注意力的图像配准方法主要针对RNN的每一步图像。然而，有些单词或短语(例如:a, of)不需要注意视觉信号。此外，这些不必要的可视信号可能会影响标题生成过程并降低整体性能。因此，他们提出的方法可以确定什么时候只关注图像区域，什么时候只关注语言生成模型。一旦它决定看图像，那么它必须选择图像的空间位置。该方法的第一个贡献是引入了一种新的空间注意方法，可以从图像中计算出空间特征。然后在他们的自适应注意方法中，引入了一个新的LSTM扩展。通常情况下，LSTM作为一个编码器，可以在每次步进时产生一个隐藏状态。然而，这个扩展能够产生一个额外的视觉标记，为解码器提供一个回退选项。它还有一个可以控制解码器从图像中获取多少信息的门。

> 虽然基于注意力的方法在生成用于图像标题的单词或短语时会寻找图像的不同区域，但是通过这些方法生成的注意力图不一定总是与图像的适当区域相对应。它会影响图像标注的性能。Liu等[84]提出了一种神经图像字幕的方法。该方法可以对注意图进行时间步长估计和校正。校正意味着在图像区域和生成的单词之间建立一致的映射。为了实现这些目标，该方法引入了一种定量评估指标来计算注意力图。 它使用Flickr30k实体数据集[113]和MS COCO [83]数据集来测量地面真相注意图和图像区域的语义标记。为了更好地学习注意力函数，提出了监督注意力模型。这里使用了两种类型的监督注意模型:具有对齐注释的强监督模型和具有语义标记的弱监督模型。在带有对齐注释模型的强大监督下，它可以直接将地面真实单词映射到区域。 但是，ground truth校准并非总是可能的，因为收集和注释数据通常非常昂贵。在MSCOCO数据集上使用边界框或分割掩码执行弱监督。实验结果表明，监督注意力模型在映射注意力和图像字幕方面有较好的效果.

> Chen等。 [21]提出了另一种基于注意力的图像字幕方法。 此方法同时考虑空间和通道注意，以计算注意映射。现有的基于注意力的图像标注方法只考虑空间信息来生成注意力图(映射)。这些空间注意方法的一个共同缺点是它们只在注意特征图上计算加权池。结果，这些方法逐渐失去了空间信息。 而且，他们仅使用来自CNN的最后一个conv层的空间信息。这一层的感受域相当大，使得各区域之间有了间隙。因此，他们没有得到显着的空间注意的图像。然而，在这种方法中，CNN的特征不仅提取自空间位置，而且提取自不同的通道和多层。因此，它得到了显著的空间注意。除此之外，在该方法中，卷积层的每个过滤器都充当语义检测器[159]，而其他方法则使用外部资源来获取语义信息。

> 为了**缩小人产生的描述与机器产生的描述之间的差距**，Tavakoli等[134]提出了一种基于注意力的图像字幕方法。这是一个基于显着性的自下而上的注意力模型，可以利用该模型与其他基于注意力的图像字幕方法进行比较。 研究发现，人类首先描述的是重要的对象而不是次要的对象。 **这也表明该方法在从未见过的数据上表现更好。**

> 大多数以前的图像字幕方法都采用自顶向下的方法来构造视觉注意图。 这些机制通常集中在从CNN的一层或两层输出中获得的一些选择性区域。输入区域的大小相同，并且接收场的形状相同。 这种方法很少考虑图像的内容。 然而，安德森等人的方法 [4]应用了自上而下和自下而上的方法。自上而下的关注机制将Faster R-CNN [116]用于可以选择图像显着区域的区域建议(图像框)。因此，此方法可以同时参与对象级别区域和其他显着图像区域。

> Park等。 [111]引入了另一种基于注意力的图像字幕方法。 该方法可以生成解决**图像个人问题的图像标题**。 它主要考虑两个任务：**主题标签预测和后生成**。此方法使用上下文序列存储网络（**CSMN**）从图像中获取上下文信息。但是，描述它们并不容易，因为它需要图像的主题，情感和背景。 因此，该方法考虑了来自先前文档的关于用户的词汇或写作风格的过去知识，以生成图像描述。 为了使用这种新型的图像字幕，CSMN方法有三个贡献：首先，该网络的内存可以用作存储库并保留多种类型的上下文信息。 其次，存储器的设计方式是它可以顺序存储所有先前生成的单词。 因此，它不存在消失的梯度问题。 第三，提出的CNN可以与多个内存插槽相关联，这有助于理解上下文概念。

> 基于注意力的方法已经在图像字幕和其他计算机视觉任务中表现出了良好的性能和效率。**然而，由这些基于注意的方法生成的注意图仅依赖于机器。他们不考虑任何来自人类注意力的监督。**这使得有必要考虑注视信息是否可以改善这些注意方法在图像字幕中的性能。**gaze**凝视表示人类对场景的认知和感知。人的凝视可以识别图像中物体的重要位置。因此，基于用户建模[18,35,109,122,124]、基于物体定位[100]或识别[67]、基于整体场景理解[158,160]的gaze机制已经显示出其潜在的性能。然而，Sugano等人[129]认为凝视信息尚未整合到图像字幕方法中。该方法利用深度神经网络的注意力机制，将人的视线引入到图像字幕的生成中。该方法将人类注视信息合并到一个基于注意力的LSTM模型中[152]。实验采用SALICON数据集[60]，取得了较好的结果。